{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_08.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNqxT6duaDgI7SqOPkrNCry"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kJd-iwdn5mC"
      },
      "source": [
        "# Lab 08-1 Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akLayD28oDnl"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG9OAZP2ra7w"
      },
      "source": [
        "## XOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YuOpPIMpWD4"
      },
      "source": [
        "**BCE Loss** : Binary Cross Entropy\n",
        "\n",
        "$$ BCE(x) = - {1\\over N} \\sum^N_{i = 1}y_ilog(h(x_i;\\theta)) + (1 - y_i) log(1-h(x_i; \\theta)) $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljsCv9wVnKi2",
        "outputId": "c2c4bf10-402e-45f7-d53e-e144039697d0"
      },
      "source": [
        "# XOR perceptron implementation\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "X = torch.FloatTensor([[0,0], [0,1], [1,0], [1,1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "# nn layers\n",
        "linear = torch.nn.Linear(2, 1, bias = True)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "model = torch.nn.Sequential(linear, sigmoid).to(device)\n",
        "\n",
        "#define cost/loss & optimizer\n",
        "criterion = torch.nn.BCELoss().to(device) # Binary classification -> BCE\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1)\n",
        "\n",
        "for step in range(10001):\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(X)\n",
        "  # cost / loss function\n",
        "  cost = criterion(hypothesis, Y)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if step % 100 == 0:\n",
        "    print(step, cost.item())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7297654151916504\n",
            "100 0.6931477189064026\n",
            "200 0.6931471824645996\n",
            "300 0.6931471824645996\n",
            "400 0.6931471824645996\n",
            "500 0.6931471824645996\n",
            "600 0.6931471824645996\n",
            "700 0.6931471824645996\n",
            "800 0.6931471824645996\n",
            "900 0.6931471824645996\n",
            "1000 0.6931471824645996\n",
            "1100 0.6931471824645996\n",
            "1200 0.6931471824645996\n",
            "1300 0.6931471824645996\n",
            "1400 0.6931471824645996\n",
            "1500 0.6931471824645996\n",
            "1600 0.6931471824645996\n",
            "1700 0.6931471824645996\n",
            "1800 0.6931471824645996\n",
            "1900 0.6931471824645996\n",
            "2000 0.6931471824645996\n",
            "2100 0.6931471824645996\n",
            "2200 0.6931471824645996\n",
            "2300 0.6931471824645996\n",
            "2400 0.6931471824645996\n",
            "2500 0.6931471824645996\n",
            "2600 0.6931471824645996\n",
            "2700 0.6931471824645996\n",
            "2800 0.6931471824645996\n",
            "2900 0.6931471824645996\n",
            "3000 0.6931471824645996\n",
            "3100 0.6931471824645996\n",
            "3200 0.6931471824645996\n",
            "3300 0.6931471824645996\n",
            "3400 0.6931471824645996\n",
            "3500 0.6931471824645996\n",
            "3600 0.6931471824645996\n",
            "3700 0.6931471824645996\n",
            "3800 0.6931471824645996\n",
            "3900 0.6931471824645996\n",
            "4000 0.6931471824645996\n",
            "4100 0.6931471824645996\n",
            "4200 0.6931471824645996\n",
            "4300 0.6931471824645996\n",
            "4400 0.6931471824645996\n",
            "4500 0.6931471824645996\n",
            "4600 0.6931471824645996\n",
            "4700 0.6931471824645996\n",
            "4800 0.6931471824645996\n",
            "4900 0.6931471824645996\n",
            "5000 0.6931471824645996\n",
            "5100 0.6931471824645996\n",
            "5200 0.6931471824645996\n",
            "5300 0.6931471824645996\n",
            "5400 0.6931471824645996\n",
            "5500 0.6931471824645996\n",
            "5600 0.6931471824645996\n",
            "5700 0.6931471824645996\n",
            "5800 0.6931471824645996\n",
            "5900 0.6931471824645996\n",
            "6000 0.6931471824645996\n",
            "6100 0.6931471824645996\n",
            "6200 0.6931471824645996\n",
            "6300 0.6931471824645996\n",
            "6400 0.6931471824645996\n",
            "6500 0.6931471824645996\n",
            "6600 0.6931471824645996\n",
            "6700 0.6931471824645996\n",
            "6800 0.6931471824645996\n",
            "6900 0.6931471824645996\n",
            "7000 0.6931471824645996\n",
            "7100 0.6931471824645996\n",
            "7200 0.6931471824645996\n",
            "7300 0.6931471824645996\n",
            "7400 0.6931471824645996\n",
            "7500 0.6931471824645996\n",
            "7600 0.6931471824645996\n",
            "7700 0.6931471824645996\n",
            "7800 0.6931471824645996\n",
            "7900 0.6931471824645996\n",
            "8000 0.6931471824645996\n",
            "8100 0.6931471824645996\n",
            "8200 0.6931471824645996\n",
            "8300 0.6931471824645996\n",
            "8400 0.6931471824645996\n",
            "8500 0.6931471824645996\n",
            "8600 0.6931471824645996\n",
            "8700 0.6931471824645996\n",
            "8800 0.6931471824645996\n",
            "8900 0.6931471824645996\n",
            "9000 0.6931471824645996\n",
            "9100 0.6931471824645996\n",
            "9200 0.6931471824645996\n",
            "9300 0.6931471824645996\n",
            "9400 0.6931471824645996\n",
            "9500 0.6931471824645996\n",
            "9600 0.6931471824645996\n",
            "9700 0.6931471824645996\n",
            "9800 0.6931471824645996\n",
            "9900 0.6931471824645996\n",
            "10000 0.6931471824645996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFosgxerrcit"
      },
      "source": [
        "# Lab 08-2 Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5UN1ZtBrlhp"
      },
      "source": [
        "To solve the problem that XOR cannot be represented with single Perceptron, we introduce the concept of \"**backpropagation**\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjm8VfQPqVY9"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "# nn Layers , the same as two of nn.Linear\n",
        "w1 = torch.Tensor(2, 2).to(device)\n",
        "b1 = torch.Tensor(2).to(device)\n",
        "w2 = torch.Tensor(2, 1).to(device)\n",
        "b2 = torch.Tensor(1).to(device)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXDJe9DZsHf5"
      },
      "source": [
        "def sigmoid(x):\n",
        "  # sigmoid function\n",
        "  return 1.0 / (1.0 + torch.exp(-x))\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQtYU9wFsNAY"
      },
      "source": [
        "def sigmoid_prime(x):\n",
        "  # derivative of the sigmoid function\n",
        "  return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZJZ7GM8ubF6"
      },
      "source": [
        "learning_rate = 1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-RUHJvNsTlm"
      },
      "source": [
        "# learning\n",
        "\n",
        "for step in range(10001):\n",
        "  # forward\n",
        "  l1 = torch.add(torch.matmul(X, w1), b1)\n",
        "  a1 = sigmoid(l1)\n",
        "  l2 = torch.add(torch.matmul(a1, w2), b2)\n",
        "  Y_pred = sigmoid(l2)\n",
        "\n",
        "  cost = -torch.mean(Y * torch.log(Y_pred) + (1- Y) * torch.log(1 - Y_pred))\n",
        "\n",
        "  # bacd prop (chan rule)\n",
        "\n",
        "  # Loss derivative\n",
        "  d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)\n",
        "   \n",
        "  \n",
        "  # layer 2\n",
        "  d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
        "  d_b2 = d_l2\n",
        "  d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n",
        "\n",
        "  # layer 1\n",
        "  d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
        "  d_l1 = d_a1 * sigmoid_prime(l1)\n",
        "  d_b1 = d_l1\n",
        "  d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
        "\n",
        "  # weight update\n",
        "  w1 = w1 - learning_rate * d_w1\n",
        "  b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
        "  w2 = w2 - learning_rate* d_w2\n",
        "  b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    print(step, cost.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLTNaAastAns",
        "outputId": "dbd626e4-0806-411c-b9ae-c485778dca64"
      },
      "source": [
        "# Backpropagation using torch\n",
        "\n",
        "# nn layers\n",
        "# MLP\n",
        "linear1 = torch.nn.Linear(2, 2, bias = True)\n",
        "linear2 = torch.nn.Linear(2, 1, bias = True)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1)\n",
        "for step in range(10001):\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(X)\n",
        "  # cost / loss function\n",
        "  cost = criterion(hypothesis, Y)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if step % 100 == 0:\n",
        "    print(step, cost.item())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.6931954026222229\n",
            "100 0.6930721998214722\n",
            "200 0.6929640173912048\n",
            "300 0.6926223039627075\n",
            "400 0.690690815448761\n",
            "500 0.6712223887443542\n",
            "600 0.5781338810920715\n",
            "700 0.5087843537330627\n",
            "800 0.3558744788169861\n",
            "900 0.12581370770931244\n",
            "1000 0.061768241226673126\n",
            "1100 0.03949028253555298\n",
            "1200 0.028697241097688675\n",
            "1300 0.02242206409573555\n",
            "1400 0.018346931785345078\n",
            "1500 0.015498388558626175\n",
            "1600 0.013400016352534294\n",
            "1700 0.01179259829223156\n",
            "1800 0.010523222386837006\n",
            "1900 0.009496381506323814\n",
            "2000 0.00864909403026104\n",
            "2100 0.007938425987958908\n",
            "2200 0.007334042806178331\n",
            "2300 0.006813946180045605\n",
            "2400 0.0063617597334086895\n",
            "2500 0.005965052638202906\n",
            "2600 0.005614331923425198\n",
            "2700 0.005302071571350098\n",
            "2800 0.005022276192903519\n",
            "2900 0.00477019976824522\n",
            "3000 0.004541928879916668\n",
            "3100 0.0043343049474060535\n",
            "3200 0.004144606180489063\n",
            "3300 0.003970608115196228\n",
            "3400 0.003810522612184286\n",
            "3500 0.0036626821383833885\n",
            "3600 0.003525795415043831\n",
            "3700 0.0033986461348831654\n",
            "3800 0.0032802748028188944\n",
            "3900 0.003169795498251915\n",
            "4000 0.0030664438381791115\n",
            "4100 0.0029695744160562754\n",
            "4200 0.002878572791814804\n",
            "4300 0.002792913932353258\n",
            "4400 0.002712163608521223\n",
            "4500 0.0026359022594988346\n",
            "4600 0.0025638299994170666\n",
            "4700 0.0024955275002866983\n",
            "4800 0.0024307251442223787\n",
            "4900 0.0023691982496529818\n",
            "5000 0.0023106771986931562\n",
            "5100 0.00225493754260242\n",
            "5200 0.002201829571276903\n",
            "5300 0.0021511437371373177\n",
            "5400 0.002102670492604375\n",
            "5500 0.002056364668533206\n",
            "5600 0.0020120469853281975\n",
            "5700 0.0019695379305630922\n",
            "5800 0.0019288372714072466\n",
            "5900 0.001889765728265047\n",
            "6000 0.0018521735910326242\n",
            "6100 0.0018161055631935596\n",
            "6200 0.001781352562829852\n",
            "6300 0.001747914357110858\n",
            "6400 0.0017157013062387705\n",
            "6500 0.001684623770415783\n",
            "6600 0.0016546815168112516\n",
            "6700 0.0016257402021437883\n",
            "6800 0.0015978296287357807\n",
            "6900 0.0015708451392129064\n",
            "7000 0.0015447568148374557\n",
            "7100 0.001519475132226944\n",
            "7200 0.0014950449112802744\n",
            "7300 0.0014713462442159653\n",
            "7400 0.001448394381441176\n",
            "7500 0.0014261594042181969\n",
            "7600 0.0014045665739104152\n",
            "7700 0.0013836456928402185\n",
            "7800 0.001363307237625122\n",
            "7900 0.0013435360742732882\n",
            "8000 0.0013243474531918764\n",
            "8100 0.0013057109899818897\n",
            "8200 0.001287582446821034\n",
            "8300 0.0012699465733021498\n",
            "8400 0.0012528032530099154\n",
            "8500 0.001236107898876071\n",
            "8600 0.001219860278069973\n",
            "8700 0.0012040010187774897\n",
            "8800 0.0011885148705914617\n",
            "8900 0.001173461670987308\n",
            "9000 0.0011587966000661254\n",
            "9100 0.0011444599367678165\n",
            "9200 0.0011304814834147692\n",
            "9300 0.0011168612400069833\n",
            "9400 0.0011035245843231678\n",
            "9500 0.001090516336262226\n",
            "9600 0.0010778066935017705\n",
            "9700 0.001065410440787673\n",
            "9800 0.0010532678570598364\n",
            "9900 0.001041423762217164\n",
            "10000 0.0010298185516148806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI2YdkZCyGIF"
      },
      "source": [
        "The loss has reduced with the wide-deep MLP!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvDieU7gxUDm",
        "outputId": "fedd0f42-5ad3-46ae-cd53-61710b151447"
      },
      "source": [
        "# XOR-nn-wide-deep\n",
        "\n",
        "# nn layers\n",
        "\n",
        "# 4 MLP\n",
        "linear1 = torch.nn.Linear(2, 10, bias = True)\n",
        "linear2 = torch.nn.Linear(10, 10, bias = True)\n",
        "linear3 = torch.nn.Linear(10, 10, bias = True)\n",
        "linear4 = torch.nn.Linear(10, 1, bias = True)\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1)\n",
        "for step in range(10001):\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(X)\n",
        "  # cost / loss function\n",
        "  cost = criterion(hypothesis, Y)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if step % 100 == 0:\n",
        "    print(step, cost.item())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.6957021951675415\n",
            "100 0.6931525468826294\n",
            "200 0.6931518316268921\n",
            "300 0.6931511163711548\n",
            "400 0.6931504011154175\n",
            "500 0.6931496858596802\n",
            "600 0.6931490302085876\n",
            "700 0.6931482553482056\n",
            "800 0.6931476593017578\n",
            "900 0.6931469440460205\n",
            "1000 0.6931463479995728\n",
            "1100 0.6931456327438354\n",
            "1200 0.6931450366973877\n",
            "1300 0.6931443214416504\n",
            "1400 0.6931436061859131\n",
            "1500 0.6931430101394653\n",
            "1600 0.693142294883728\n",
            "1700 0.6931415796279907\n",
            "1800 0.6931408643722534\n",
            "1900 0.6931401491165161\n",
            "2000 0.6931394934654236\n",
            "2100 0.6931387186050415\n",
            "2200 0.6931378841400146\n",
            "2300 0.6931371688842773\n",
            "2400 0.6931363344192505\n",
            "2500 0.6931354403495789\n",
            "2600 0.6931345462799072\n",
            "2700 0.6931335926055908\n",
            "2800 0.6931326985359192\n",
            "2900 0.693131685256958\n",
            "3000 0.693130612373352\n",
            "3100 0.6931294202804565\n",
            "3200 0.693128228187561\n",
            "3300 0.6931269764900208\n",
            "3400 0.6931256055831909\n",
            "3500 0.6931241750717163\n",
            "3600 0.6931227445602417\n",
            "3700 0.6931210160255432\n",
            "3800 0.6931192874908447\n",
            "3900 0.6931174397468567\n",
            "4000 0.6931154131889343\n",
            "4100 0.6931131482124329\n",
            "4200 0.6931107044219971\n",
            "4300 0.6931081414222717\n",
            "4400 0.6931052207946777\n",
            "4500 0.6931020021438599\n",
            "4600 0.6930984854698181\n",
            "4700 0.6930946111679077\n",
            "4800 0.6930902004241943\n",
            "4900 0.6930853724479675\n",
            "5000 0.6930798292160034\n",
            "5100 0.693073570728302\n",
            "5200 0.693066418170929\n",
            "5300 0.6930581331253052\n",
            "5400 0.6930485963821411\n",
            "5500 0.6930372714996338\n",
            "5600 0.6930240392684937\n",
            "5700 0.6930081844329834\n",
            "5800 0.6929888725280762\n",
            "5900 0.692965030670166\n",
            "6000 0.6929352283477783\n",
            "6100 0.6928971409797668\n",
            "6200 0.6928471326828003\n",
            "6300 0.6927797794342041\n",
            "6400 0.692685604095459\n",
            "6500 0.6925481557846069\n",
            "6600 0.6923357844352722\n",
            "6700 0.6919821500778198\n",
            "6800 0.6913288831710815\n",
            "6900 0.6899222731590271\n",
            "7000 0.6860334277153015\n",
            "7100 0.6688233613967896\n",
            "7200 0.5522720217704773\n",
            "7300 0.4421425461769104\n",
            "7400 0.015378246083855629\n",
            "7500 0.006717183627188206\n",
            "7600 0.004115208052098751\n",
            "7700 0.002911937888711691\n",
            "7800 0.002230703830718994\n",
            "7900 0.0017966360319405794\n",
            "8000 0.0014977001119405031\n",
            "8100 0.0012802028795704246\n",
            "8200 0.0011154150124639273\n",
            "8300 0.0009864689782261848\n",
            "8400 0.0008830293081700802\n",
            "8500 0.0007983915274962783\n",
            "8600 0.0007278681732714176\n",
            "8700 0.0006682353559881449\n",
            "8800 0.000617269251961261\n",
            "8900 0.0005732240970246494\n",
            "9000 0.0005347271799109876\n",
            "9100 0.0005008983425796032\n",
            "9200 0.0004709020722657442\n",
            "9300 0.0004441567580215633\n",
            "9400 0.0004201700212433934\n",
            "9500 0.00039847963489592075\n",
            "9600 0.00037890655221417546\n",
            "9700 0.00036101826117374003\n",
            "9800 0.000344710482750088\n",
            "9900 0.00032975937938317657\n",
            "10000 0.00031597117776982486\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}