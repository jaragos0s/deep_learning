{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS231n_lecture13.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPSohbhICgdGFQKGCKAUefo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pHtrgTwGDSbJ"},"source":["# Lecture 13 Generative Models"]},{"cell_type":"markdown","metadata":{"id":"zXEYucmZEI7g"},"source":["Given training data, generate new samples from same distribution"]},{"cell_type":"markdown","metadata":{"id":"BTVoMxf3VXIe"},"source":["##PixelCNN\n","Define tractable density function, optimize likelihood of training data:\n","$$ p_\\theta(x) = \\prod_{i=1}^n p_\\theta(x_i|x_1, ... , x_{i-1}) $$\n"]},{"cell_type":"markdown","metadata":{"id":"k6jGrCsXWHBT"},"source":["##VAE\n","Define intractable density function with latent **z**:\n","$$p_\\theta(x) = \\int p_\\theta(z)p_\\theta(x|z)dz$$\n","Cannot optimize directly, derive and optimize lower bound on likelihood instead"]},{"cell_type":"markdown","metadata":{"id":"Y4vzoXTekGsJ"},"source":["## GAN\n","Sample from a simple distribution, e.g. random noise\n","\n","Input : random noise, Output : sample from training distribution\n","\n","**Generator Network**: try to fool the discriminator by generating real-looking images\n","**Discriminator Network**: try to distinguish between real and fake images"]},{"cell_type":"markdown","metadata":{"id":"duHBmmc0llOQ"},"source":["**minmax objective function**\n","$$min_{\\theta_g} max_{\\theta_d} [\\mathbb{E}_{x~p_{data}} logD_{\\theta_d}(x) + \\mathbb{E}_{z\\sim p(z)} log(1 - D_{\\theta_d}(G_{\\theta_g}(z)))]$$\n","\n","1. Gradient ascent on discriminator\n","$$max_{\\theta_d} [\\mathbb{E}_{x~p_{data}} logD_{\\theta_d}(x) + \\mathbb{E}_{z\\sim p(z)} log(1 - D_{\\theta_d}(G_{\\theta_g}(z)))]$$\n","\n","2. Gradient descent on generator\n","$$min_{\\theta_g}\\mathbb{E}_{z\\sim p(z)} log(1 - D_{\\theta_d}(G_{\\theta_g}(z)))$$"]},{"cell_type":"code","metadata":{"id":"rlz3fW4mDPId"},"source":["# pseudo codes of GAN training algorithm\n","'''\n","for number of training iterations do\n","  for k steps do\n","    sample minibatch of m noise samples {z^(1), ... , z^(m)} from noise prior p_g(z)\n","    sample minibatch of m examples {x^(1), ... , x^(m)} from data generating distribution p_data(x)\n","    update the discriminator by ascending its stochastic gradient\n","  end for\n","  sample minibatch of m noise samples {z^(1), ... , z~(m)} from noise prior p_g(z)\n","  update the generator by ascending its stochastic gradient (improved objective)\n","end for\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C1nwjuSapQVt"},"source":["##DCGAN\n","- Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator)\n","- Use batchnorm in both the generator and the discriminator\n","- Remove fully connected hidden lyaers for deeper architectures\n","- Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n","- Use LeakyReLU activation in the discriminator for all layers"]}]}