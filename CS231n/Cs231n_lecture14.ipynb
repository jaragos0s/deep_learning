{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cs231n_lecture14.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOEcS2sRXCv8cwj2fe9ZZUS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RW5wR6pPwLW5"},"source":["# Lecture 14 Deep Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"BKVoNXVgxQ4b"},"source":["## Markov Decision Process\n","\n","**Marcov property** \n","\n","Current state completely characterises the state of the world\n","\n","$$ (\\mathcal{S}, \\mathcal{A}, \\mathcal{R}, \\mathbb{P}, \\gamma) $$\n","\n","$\\mathcal{S}$ : set of possible states\n","\n","$\\mathcal{A}$ : set of possible actions\n","\n","$\\mathcal{R}$ : distribution of reward given (state, action) pair\n","\n","$\\mathbb{P}$ : transition probability\n","\n","$\\gamma$ : discount factor\n"]},{"cell_type":"markdown","metadata":{"id":"WyFCaNJ8yse3"},"source":["A policy $\\pi$ is a function from S to A that specifies what action to take in each state"]},{"cell_type":"markdown","metadata":{"id":"J44p4O1e0N-6"},"source":["**The optimal Q-value function $Q^*$**\n","is the maxinum expected cumulative reward achievable from a given (state, action) pair;\n","$$ Q^*(s,a) = max_{\\pi} \\mathbb{E} \\left[ \\sum_{t \\ge 0} \\gamma^t r_t | s_0 = s, a_0 = a, \\pi \\right] $$"]},{"cell_type":"markdown","metadata":{"id":"gBXZHggf1G-i"},"source":["**Bellman Equation**\n","$$ Q^*(s, a) = \\mathbb{E}_{s'} \\sim \\epsilon \\left[ r + \\gamma= max_{a'} Q^*(s', a') | s, a \\right] $$"]},{"cell_type":"markdown","metadata":{"id":"RYXF4KF32MHK"},"source":["## Q-learning"]},{"cell_type":"code","metadata":{"id":"GLveUllvv_BV"},"source":["# Algorithm of Deep Q-learning with Experience Replay\n","'''\n","Initialize replay memory D to capacity N\n","Initialize action-value function Q with random weights\n","for episode = 1, M do\n","  Initialize sequence s_1 = {x_1} and preprocessed sequenced pi_1 = pi(s_1)\n","  for t = 1, T do\n","    With probability epsilon select a random action a_t\n","    otherwise select a_t = max_a(Q^* (pi(s_t), a; theta)\n","    Execute action a_t in emulator and observe reward r_t and image x_{t+1}\n","    Set s_{t+1} = s_t, a_t, x_{t+1} and preprocess pi_{t+1} = pi(s_{t+1})\n","    Store transition (pi_t, a_t, r_t, pi_{t+1}) in D\n","    Sample random minibatch of transitions (pi_j, a_j, r_j, pi_{j+1}) from D\n","    Set y_j = r_j for terminal pi_{j+1}\n","        y_j = r_j + gamma*max_a' Q(pi_{j+1}, a' ; theta) for non-terminal pi_{j+1}\n","    Perform a gradient descent step on (y_j - Q(pi_j, a_j ; theta))^2\n","  end for\n","end for\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qW4hxgxK6rgr"},"source":["## REINFORCE algorithm\n","\n","Expected reward: \n","$$ J(\\theta) = \\mathbb{E}_{\\tau \\sim p(\\tau ; \\theta) } [r(\\tau)]\n","= \\int_r r(\\tau) p(\\tau;\\theta) d\\tau\n","$$\n","\n","Intractable -> use trick\n","\n","$$ \\triangledown_\\theta p(\\tau;\\theta) = p(\\tau;\\theta) \\triangledown_\\theta p(\\tau;\\theta) / p(\\tau;\\theta) = p(\\tau;\\theta) \\triangledown_\\theta log p(\\tau; \\theta) $$\n","\n","Inject this back:\n","$$ \\triangledown_\\theta J(\\theta) = \\int_r (rlog p(\\tau; \\theta)) p(\\tau;\\theta) d\\tau = \\mathbb{E}_{\\tau \\sim p(\\tau; \\theta)} [ r(\\tau) \\triangledown_\\theta logp(\\tau ; \\theta)] $$\n","\n","\n","-> Can estimate with Monte Carlo sampling"]}]}