{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS231n_lecture3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVsc0tzh6BZp2S2qhkZfKD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD13dm_L3zUX"
      },
      "source": [
        "#Lecture3 Loss Functions and Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ03X0ED5Mvg"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbcPn5rf9cPA"
      },
      "source": [
        "#Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FneuPi1S4AJe"
      },
      "source": [
        "Multiclass SVM Loss\n",
        "$$ L_i = \\sum_{j != y_i} max(0, s_j -s_{y_i} + 1)$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeiF_L673v_7"
      },
      "source": [
        "def L_i_vectorized(x, y, W):\n",
        "  scores = W.dot(x)\n",
        "  margins = np.maximum(0, scores - scores[y] + 1)\n",
        "  margins[y] = 0 # max로 나온 결과에서 정답 클래스만 0으로 만들어 줌\n",
        "  loss_i = np.sum(margins)\n",
        "  return loss_i "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8--ftSz9eYg"
      },
      "source": [
        "Softmax : We use log since log function is monotonic increasing. (To make it easy)\n",
        "\n",
        "Also, we want to how \"bad\" it is, we add \"minus\"\n",
        " \n",
        "$$ L_i = -log(e^{s_{y_i}} / \\sum_j e^{s_j})$$\n",
        "\n",
        "Min loss : 0, Max loss : ∞\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE9B0vxz67b6"
      },
      "source": [
        "## Regularization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nivg9AE6-7H"
      },
      "source": [
        "L2 Regularization : Give penalty to  Euclidean Norm of the weighted matrix W\n",
        "\n",
        "$$ R(W) = \\sum_{k}\\sum_{l}W^2_{k, l} $$\n",
        "\n",
        "L1 Regularization : Make matrix W a sparse matrix\n",
        "\n",
        "$$ R(W) = \\sum_{k}\\sum_{l}|W_{k, l}| $$\n",
        "\n",
        "Elastic net (L1 + L2) :\n",
        "\n",
        "$$ R(W) = \\sum_{k}\\sum_{l}|\\beta W^2_{k, l} + W_{k, l}| $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O197812BCjD"
      },
      "source": [
        "##Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQGPbrZJC7ZV"
      },
      "source": [
        "- Numerical gradient : approximate, slow, easy to write\n",
        "- Analytic gradient : exact, fast, error-prone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEhVl0tvC7Nv"
      },
      "source": [
        "Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NesH9mV6-gy"
      },
      "source": [
        "# Vanilla Gradient Descent\n",
        "\n",
        "while True:\n",
        "  weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
        "  weights += - step_size * weights_grad # perform parameter update \n",
        "  # gradient가 함수에서 증가하는 방향 -> -gradient를 해야 내려가는 방향\n",
        "  # step size : hyperparameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA2oLT59ESeZ"
      },
      "source": [
        "Stochastic Gradient Descent (SGD)\n",
        "\n",
        "$$ L(W ) = 1/N \\sum_{i = 1}^N L_i(x_i, y_i, W) + \\lambda R(W) $$\n",
        "\n",
        "Approximate sum using a minibatch\n",
        "$$ ∇_W L(W) = 1/N \\sum_{i = 1}^N ∇_W L_i(x_i, y_i, W) + \\lambda ∇_W R(W)  $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl8ks_--66t1"
      },
      "source": [
        "# vanilla Minibatch Gradient Descent\n",
        "\n",
        "while True:\n",
        "  data_batch = sample_training_dat(data, 256) # sample 256 examples\n",
        "  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n",
        "  weights += -step_size * weights_grad # perform parameter update"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}