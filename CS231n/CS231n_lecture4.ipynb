{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS231n_lecture4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8Lz6gB0Nk1ya+Xp1P6L+t"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y69RAyd29Pm4"
      },
      "source": [
        "#Lecture4. Introduction to Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG0kBdBPtWFa"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "recursive application of the chain rule along a computational graph to compute the gradients of all inputs/parameters/intermediates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLdAQbsk9KWF"
      },
      "source": [
        "class MultiplyGate(object):\n",
        "  def forward(x, y):\n",
        "    z = x * y\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    return z\n",
        "\n",
        "  def backward(dz):\n",
        "    dx = self.y * dz # [dz/dx * dL/dz]\n",
        "    dy = self.x * dz # [dz/dy * dL/dz]\n",
        "    reuturn [dx, dy]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma9WYtUz8KJR"
      },
      "source": [
        "## Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePb_aXJY8O5g"
      },
      "source": [
        "# Full implementation of training a 2-layer Neural Network\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x, y = randn(N, D_in), randn(N, D_out)\n",
        "w1, w2 = randn(D_in, H), randn(H, D_out)\n",
        "\n",
        "for t in rangd(2000):\n",
        "  h = 1 / (1 + np.exp(-x.dot(w1)))\n",
        "  y_pred = h.dot(w2)\n",
        "  loss = np.square(y_pred - y).sum()\n",
        "  print(t, loss)\n",
        "\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_w2 = h.T.dot(grad_y_pred)\n",
        "  grad_h = grad_y_pred.dot(w2.T)\n",
        "  grad_w1 = x.T.dot(grad_h * h * (1 - h))\n",
        "\n",
        "  w1 -= 1e-4 * grad_w1\n",
        "  w2 -= 1e-4 * grad_w2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxCTWGNvAywZ"
      },
      "source": [
        "class Neuron:\n",
        "  # ...\n",
        "  def neuron_tick(inputs):\n",
        "    cell_body_sum = np.sum(inputs * self.weights) + self.bias\n",
        "    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function\n",
        "    return firing_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnG4A1-OBLBF"
      },
      "source": [
        "# forward-pass of a 3-layer neural network:\n",
        "\n",
        "\n",
        "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function\n",
        "x = np.random.randn(3, 1) # random input vector of three numbers (3 x 1)\n",
        "h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4 x 1)\n",
        "h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4 x 1)\n",
        "out = np.dot(W3, h2) + b3 # output neuron (1 x 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}