{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어처리.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1_M9kLecg6wjcJY3rfeJzvWAeqSkLwiH0",
      "authorship_tag": "ABX9TyMp/82mvJYG4VQ4MHEAeQ81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaragos0s/deep_learning/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFm--jcXWi60"
      },
      "source": [
        "핸즈온 머신러닝 \r\n",
        "#Chapter16. RNN과 어텐션을 사용한 자연어 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN5XzSy7cANj",
        "outputId": "cd036abd-e59d-4c18-cfa3-cd8fad837e16"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIRtfs83XhMp"
      },
      "source": [
        "##Char-RNN을 사용해 셰익스피어 문체 생성하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBY3AtOrYF9R"
      },
      "source": [
        "import sys\r\n",
        "import sklearn\r\n",
        "assert sklearn.__version__ >= \"0.20\"\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "assert tf.__version__ >= \"2.0\"\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "import matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEnp_JOjWcuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad7e4e2-b761-4ab8-e091-60d8f0312e02"
      },
      "source": [
        "'''작품 다운로드'''\r\n",
        "shakespeare_url = \"https://homl.info/shakespeare\"\r\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\r\n",
        "with open(filepath) as f:\r\n",
        "  shakespeare_text = f.read()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THwZNs5eY0kt",
        "outputId": "0856518f-a9ee-4f72-d457-821b138d765e"
      },
      "source": [
        "print(shakespeare_text[:200])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HltcmYK5ZI4i",
        "outputId": "bbde5162-1a47-4310-f178-42a0c459482b"
      },
      "source": [
        "'''shakespeare.txt 파일에 어떤 문자가 있는지 sorting 해서 출력'''\r\n",
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1pAdu2LZJ_e"
      },
      "source": [
        "''' 모든 글자를 정수로 인코딩 '''\r\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level = True) # char_level = True -> 단어 수준 인코딩(기본적으로 텍스트를 소문자로 바꿈)\r\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujzz2m_nZuFH",
        "outputId": "16eb5f36-2e3d-4de2-f9b9-6bead4da935f"
      },
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xn_8_WiZyfb"
      },
      "source": [
        "max_id = len(tokenizer.word_index) # 고유 글자 개수\r\n",
        "dataset_size = tokenizer.document_count # 전체 글자 개수"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfABYsJLZ_Cf"
      },
      "source": [
        "''' 전체 텍스트를 인코딩해 각 글자를 ID로 나타냄 '''\r\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iJlL13gaQSG"
      },
      "source": [
        "### 순차 데이터셋 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gniZw7aaX6N"
      },
      "source": [
        "훈련 세트, 검증 세트, 테스트 세트가 중복되지 않도록 만들어야 한다. \r\n",
        "\r\n",
        "시계열 데이터는 보통 시간에 따라 나눈다. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWOY3AroaUyW"
      },
      "source": [
        "''' 텍스트의 처음 90%를 훈련 세트로 사용 '''\r\n",
        "train_size = dataset_size * 90 // 100\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmMtOchla1aN"
      },
      "source": [
        "### 순차 데이터를 윈도 여러 개로 자르기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak1tIGEvqijP"
      },
      "source": [
        "데이터셋의 window() 메서드를 사용해 긴 시퀀스를 작은, 많은 텍스트 윈도(매우 짧은 부분 문자열)로 변환\r\n",
        "\r\n",
        "RNN은 이 부분 문자열 길이만큼만 역전파를 위해 펼쳐진다. -> TBPTT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGgLjyizaPda"
      },
      "source": [
        "n_steps = 100\r\n",
        "window_length = n_steps + 1 # target = 1글자 앞의 input"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjmM3geFaFrq"
      },
      "source": [
        "dataset = dataset.window(window_length, shift = 1, drop_remainder = True) \r\n",
        "#shift = 1 : 가장 큰 훈련세트를 만듦.\r\n",
        "# drop_remainder = True : 모든 윈도가 동일하게 101개의 글자를 포함하도록 설정"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bB1QtAesNRn"
      },
      "source": [
        "- window() : 각각 하나의 데이터셋으로 표현되는 윈도를 포함하는 데이터셋 생성. -> nested dataset(데이터셋 메서드를 호출하여 각 윈도를 변환할 때 유용)\r\n",
        "\r\n",
        "BUT 모델은 데이터셋이 아니라 \"텐서\"를 기대하기 때문에 훈련에 중첩 데이터셋을 바로 사용할 수 없음 -> flat dadtaset(데이터셋이 들어 있지 않는 데이터셋)으로 변환해야 함. \r\n",
        "\r\n",
        "- flat_map() : 중첩 데이터셋을 평평하게 만들기 전에 각 데이터셋에 적용할 변환 함수를 매개변수로 받을 수 있음. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-48tqjur9hB"
      },
      "source": [
        "dataset = dataset.flat_map(lambda window:window.batch(window_length)) # window_length만큼 읽고 lambda 함수를 적용해 텐서 반환 "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oshnc9Nxt-rL"
      },
      "source": [
        "''' 결과값 유지 위해 시드 설정 '''\r\n",
        "np.random.seed(42)\r\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oEG6hdzuKxf"
      },
      "source": [
        "batch_size = 32\r\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\r\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBHgEbhRutRj"
      },
      "source": [
        "''' 일반적으로 categorical 입력 특성은 one-hot vector 이나 embedding으로 인코딩 '''\r\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth = max_id), Y_batch))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxZOZYI_vKMs"
      },
      "source": [
        "# 데이터셋에 필요한 데이터를 미리 불러오기\r\n",
        "''' CPU 연산속도 < 메모리 접근속도 가 되어 CPU가 놀게 되는 비효율적인 현상 방지하기 위해\r\n",
        "    prefetch를 이용해 cache나 main memory에서 데이터를 찾음. '''\r\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GU5d3o-vqDX"
      },
      "source": [
        "### Creating & Training the Char-RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2RBv45JvNyO",
        "outputId": "2878f09d-bba6-41a2-a30a-78bf5215a83b"
      },
      "source": [
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\r\n",
        "    keras.layers.GRU(128, return_sequences=True,dropout=0.2, recurrent_dropout=0.2),\r\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\r\n",
        "# label이 정수형일 때 sparse_categorical_crossentropy 사용. \r\n",
        "# label이 one-hot vector일 때에는 categorical_crossentropy 사용\r\n",
        "# 우리는 label을 one-hot-vector로 만든게 아니라서 sparse_categorical_crossentropy로 지정\r\n",
        "\r\n",
        "'''아 짱 오래 걸림...... 오늘 안에 끝날까... 예측 하고 싶은데 ...'''\r\n",
        "\r\n",
        "history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/10\n",
            " 1501/31370 [>.............................] - ETA: 4:46:03 - loss: 2.3467"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b5gHCRkxoGt"
      },
      "source": [
        "### Char-RNN 모델 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQpJGbGdxHw1"
      },
      "source": [
        "'''전처리'''\r\n",
        "def preprocess(texts):\r\n",
        "  X = np.array(tokenizer.texts_to_sequences(texts)) - 1\r\n",
        "  return tf.one_hot(X, max_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMskfBhvx7BF"
      },
      "source": [
        "X_new = preprocess[(\"I love yo\"])\r\n",
        "Y_pred = model.predict_classes(X_new)\r\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 첫번째 문장, 마지막 글자"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pei7glwn1Rbc"
      },
      "source": [
        "### 가짜 셰익스피어 텍스트 생성하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tng0Q3WyNiq"
      },
      "source": [
        "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()\n",
        "# we can select next character randomly based on the probablity that model predicts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jstuLbU1POE"
      },
      "source": [
        "def next_char(text, temperature = 1): # control the variety of generated texts\n",
        "  X_new = preprocess([text])\n",
        "  y_proba = model.predict(X_new)[0, -1:, :]\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "  return tokenizer.sequences_to_texts(char_id.numy())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "569c0Y-qPYEO"
      },
      "source": [
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVWxKx-FX-f2"
      },
      "source": [
        "### 상태가 있는 RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEvhlNbYYLPp"
      },
      "source": [
        "RNN이 한 훈련 배치를 처리한 후에 마지막 상태를 다음 훈련 배치의 초기 상태로 사용 \r\n",
        "\r\n",
        "-> 역전파는 짧은 시퀀스에서 일어나지만 모델이 장기간 패턴을 학습할 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W5rcrbI3w-p"
      },
      "source": [
        "순차적이고 겹치지 않는 입력 시퀀스 생성(shift = n_steps) \r\n",
        "-> 하나의 윈도를 갖는 배치를 만들기\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjIrd2zBXfOF"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset = dataset.window(window_length, shift = n_steps, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.batch(1)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth_max = max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kEsYGiR5kb3"
      },
      "source": [
        "1. 각 순환층을 만들 때 stateful = True로 지정\r\n",
        "2. 상태가 있는 RNN은 배치 크기를 알아야 함. -> 첫번째 층에 batch_input_shape 매개변수 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7diEE3E4bsg"
      },
      "source": [
        "model = keras.models.Sequential([\r\n",
        "                                 keras.layers.GRU(128, return_sequences=True, stateful = True, \r\n",
        "                                                  dropout = 0.2, recurrent_dropout = 0.2, \r\n",
        "                                                  batch_input_shape = [batch_size, None, max_id]),\r\n",
        "                                 keras.layers.GRU(128, return_sequences=True, stateful = True, \r\n",
        "                                                  dropout = 0.2, recurrent_dropout = 0.2). \r\n",
        "                                 keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6iIAI6p5gBP"
      },
      "source": [
        "#epoch 끝마다 텍스트를 다시 시작하기 전에 상태 재설정\r\n",
        "class ResetStatesCallback(keras.callbacks.Callback):\r\n",
        "  def on_epoch_begin(self, epoch, logs):\r\n",
        "    self.model.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PWxuTqp6Bpv"
      },
      "source": [
        "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer=\"adam\")\r\n",
        "model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs = 50, callbacks=[ResetStatesCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5BBimLk6S_4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}