{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어처리.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN7SPxHmclR8lR/ayA7laEW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaragos0s/deep_learning/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFm--jcXWi60"
      },
      "source": [
        "핸즈온 머신러닝 \r\n",
        "#Chapter16. RNN과 어텐션을 사용한 자연어 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN5XzSy7cANj",
        "outputId": "2e8e8716-fd63-44c8-8ce3-48cc72de6cb9"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIRtfs83XhMp"
      },
      "source": [
        "##Char-RNN을 사용해 셰익스피어 문체 생성하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBY3AtOrYF9R"
      },
      "source": [
        "import sys\r\n",
        "import sklearn\r\n",
        "assert sklearn.__version__ >= \"0.20\"\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "assert tf.__version__ >= \"2.0\"\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "import matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEnp_JOjWcuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a9c9da-fd27-405d-d7ef-23bf2eb93dd8"
      },
      "source": [
        "'''작품 다운로드'''\r\n",
        "shakespeare_url = \"https://homl.info/shakespeare\"\r\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\r\n",
        "with open(filepath) as f:\r\n",
        "  shakespeare_text = f.read()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THwZNs5eY0kt",
        "outputId": "ed04abe9-8c16-4f77-9b9c-7ca4a73205a5"
      },
      "source": [
        "print(shakespeare_text[:200])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HltcmYK5ZI4i",
        "outputId": "48324e99-15a0-4a47-a1df-e2ede3abaee8"
      },
      "source": [
        "'''shakespeare.txt 파일에 어떤 문자가 있는지 sorting 해서 출력'''\r\n",
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1pAdu2LZJ_e"
      },
      "source": [
        "''' 모든 글자를 정수로 인코딩 '''\r\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level = True) # char_level = True -> 단어 수준 인코딩(기본적으로 텍스트를 소문자로 바꿈)\r\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujzz2m_nZuFH",
        "outputId": "e170c00d-d3ef-45a4-9484-6572156aa209"
      },
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xn_8_WiZyfb"
      },
      "source": [
        "max_id = len(tokenizer.word_index) # 고유 글자 개수\r\n",
        "dataset_size = tokenizer.document_count # 전체 글자 개수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfABYsJLZ_Cf"
      },
      "source": [
        "''' 전체 텍스트를 인코딩해 각 글자를 ID로 나타냄 '''\r\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iJlL13gaQSG"
      },
      "source": [
        "### 순차 데이터셋 나누기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gniZw7aaX6N"
      },
      "source": [
        "훈련 세트, 검증 세트, 테스트 세트가 중복되지 않도록 만들어야 한다. \r\n",
        "\r\n",
        "시계열 데이터는 보통 시간에 따라 나눈다. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWOY3AroaUyW"
      },
      "source": [
        "''' 텍스트의 처음 90%를 훈련 세트로 사용 '''\r\n",
        "train_size = dataset_size * 90 // 100\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmMtOchla1aN"
      },
      "source": [
        "### 순차 데이터를 윈도 여러 개로 자르기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak1tIGEvqijP"
      },
      "source": [
        "데이터셋의 window() 메서드를 사용해 긴 시퀀스를 작은, 많은 텍스트 윈도(매우 짧은 부분 문자열)로 변환\r\n",
        "\r\n",
        "RNN은 이 부분 문자열 길이만큼만 역전파를 위해 펼쳐진다. -> TBPTT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGgLjyizaPda"
      },
      "source": [
        "n_steps = 100\r\n",
        "window_length = n_steps + 1 # target = 1글자 앞의 input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjmM3geFaFrq"
      },
      "source": [
        "dataset = dataset.window(window_length, shift = 1, drop_remainder = True) \r\n",
        "#shift = 1 : 가장 큰 훈련세트를 만듦.\r\n",
        "# drop_remainder = True : 모든 윈도가 동일하게 101개의 글자를 포함하도록 설정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bB1QtAesNRn"
      },
      "source": [
        "- window() : 각각 하나의 데이터셋으로 표현되는 윈도를 포함하는 데이터셋 생성. -> nested dataset(데이터셋 메서드를 호출하여 각 윈도를 변환할 때 유용)\r\n",
        "\r\n",
        "BUT 모델은 데이터셋이 아니라 \"텐서\"를 기대하기 때문에 훈련에 중첩 데이터셋을 바로 사용할 수 없음 -> flat dadtaset(데이터셋이 들어 있지 않는 데이터셋)으로 변환해야 함. \r\n",
        "\r\n",
        "- flat_map() : 중첩 데이터셋을 평평하게 만들기 전에 각 데이터셋에 적용할 변환 함수를 매개변수로 받을 수 있음. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-48tqjur9hB"
      },
      "source": [
        "dataset = dataset.flat_map(lambda window:window.batch(window_length)) # window_length만큼 읽고 lambda 함수를 적용해 텐서 반환 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oshnc9Nxt-rL"
      },
      "source": [
        "''' 결과값 유지 위해 시드 설정 '''\r\n",
        "np.random.seed(42)\r\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oEG6hdzuKxf"
      },
      "source": [
        "batch_size = 32\r\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\r\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBHgEbhRutRj"
      },
      "source": [
        "''' 일반적으로 categorical 입력 특성은 one-hot vector 이나 embedding으로 인코딩 '''\r\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth = max_id), Y_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxZOZYI_vKMs"
      },
      "source": [
        "# 데이터셋에 필요한 데이터를 미리 불러오기\r\n",
        "''' CPU 연산속도 < 메모리 접근속도 가 되어 CPU가 놀게 되는 비효율적인 현상 방지하기 위해\r\n",
        "    prefetch를 이용해 cache나 main memory에서 데이터를 찾음. '''\r\n",
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GU5d3o-vqDX"
      },
      "source": [
        "### Creating & Training the Char-RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2RBv45JvNyO",
        "outputId": "fe0bea58-a596-4b1d-ab71-75bb8ce41bfc"
      },
      "source": [
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\r\n",
        "    keras.layers.GRU(128, return_sequences=True,dropout=0.2, recurrent_dropout=0.2),\r\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\r\n",
        "# label이 정수형일 때 sparse_categorical_crossentropy 사용. \r\n",
        "# label이 one-hot vector일 때에는 categorical_crossentropy 사용\r\n",
        "# 우리는 label을 one-hot-vector로 만든게 아니라서 sparse_categorical_crossentropy로 지정\r\n",
        "\r\n",
        "'''아 짱 오래 걸림...... 오늘 안에 끝날까... 예측 하고 싶은데 ...'''\r\n",
        "\r\n",
        "history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 6754/31370 [=====>........................] - ETA: 3:02:32 - loss: 1.8766"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b5gHCRkxoGt"
      },
      "source": [
        "### Char-RNN 모델 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQpJGbGdxHw1"
      },
      "source": [
        "'''전처리'''\r\n",
        "def preprocess(texts):\r\n",
        "  X = np.array(tokenizer.texts_to_sequences(texts)) - 1\r\n",
        "  return tf.one_hot(X, max_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMskfBhvx7BF"
      },
      "source": [
        "X_new = preprocess[(\"I love yo\"])\r\n",
        "Y_pred = model.predict_classes(X_new)\r\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 첫번째 문장, 마지막 글자"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pei7glwn1Rbc"
      },
      "source": [
        "### 가짜 셰익스피어 텍스트 생성하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tng0Q3WyNiq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jstuLbU1POE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}